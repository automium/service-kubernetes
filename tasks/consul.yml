---

- name: consul | render consul_check.sh
  set_fact:
    consul_check_sh: |
      #!/bin/bash
      status="$(kubectl --kubeconfig /root/.kube/config get nodes {{ ansible_hostname }} | awk 'NR > 1' | awk '{print $2}')"
      echo $status
      if [[ $status == "NotReady" ]]; then
       echo "Node $HOSTNAME dead"
       exit 1
      elif [[ $status == "Ready" ]]; then
       echo "Node is OK"
       exit 0
      else
        echo not found
        exit 3
      fi

- name: consul | create consul_check.sh
  copy:
    content: "{{ consul_check_sh }}"
    dest: /usr/local/bin/kubernetes-check-node
    mode: 0555
    backup: yes

- name: consul | create etcd-consul-check
  copy:
    src: etcd-consul-check.py
    dest: /usr/local/bin/etcd-consul-check
    mode: 0555
    backup: yes

- name: consul | render consul service
  set_fact:
    consul_service_render: |
      - name: "{{ identity }}"
      - name: "{{ identity }}-all"
        checks:
      {% if etcd == 'true' %}
          - args: ["/usr/local/bin/etcdctl", "--ca-file=/etc/ssl/etcd/ssl/ca.pem", "--cert-file=/etc/ssl/etcd/ssl/member-{{ ansible_hostname }}.pem", "--key-file=/etc/ssl/etcd/ssl/member-{{ ansible_hostname }}-key.pem", "--endpoint=https://127.0.0.1:2379", "cluster-health", "|", "grep", "{{ ansible_default_ipv4['address']  }}", "|", "grep", "healthy"]
            interval: "10s"
          - args: ["/usr/local/bin/etcd-consul-check", "--ca=/etc/ssl/etcd/ssl/ca.pem", "--cert-file=/etc/ssl/etcd/ssl/member-{{ ansible_hostname }}.pem", "--key=/etc/ssl/etcd/ssl/member-{{ ansible_hostname }}-key.pem"]
            interval: "10s"
      {% endif %}
      {% if master == 'true' %}
          - args: ["/usr/local/bin/kubernetes-check-node"]
            interval: "30s"
          - tcp: localhost:6443
            interval: 10s
            timeout: 3s
          - tcp: localhost:10251
            interval: 10s
            timeout: 3s
          - tcp: localhost:10252
            interval: 10s
            timeout: 3s
      {% endif %}
      {% if node == 'true' or master == 'true' %}
          - tcp: localhost:10250
            interval: 10s
            timeout: 1s
          - http: http://localhost:10248/healthz
            method: "GET"
            interval: 30s
            timeout: 5s
          - tcp: localhost:10256
            interval: 10s
            timeout: 1s
      {% endif %}
      {% if etcd == 'true' %}
      - name: "{{ cluster_name }}-etcd-cluster"
        checks:
          - args: ["/usr/local/bin/etcdctl", "--ca-file=/etc/ssl/etcd/ssl/ca.pem", "--cert-file=/etc/ssl/etcd/ssl/member-{{ ansible_hostname }}.pem", "--key-file=/etc/ssl/etcd/ssl/member-{{ ansible_hostname }}-key.pem", "--endpoint=https://127.0.0.1:2379", "cluster-health", "|", "grep", "{{ ansible_default_ipv4['address']  }}", "|", "grep", "healthy"]
            interval: "10s"
      - name: "{{ cluster_name }}-etcd"
        checks:
          - args: ["/usr/local/bin/etcd-consul-check", "--ca=/etc/ssl/etcd/ssl/ca.pem", "--cert-file=/etc/ssl/etcd/ssl/member-{{ ansible_hostname }}.pem", "--key=/etc/ssl/etcd/ssl/member-{{ ansible_hostname }}-key.pem"]
            interval: "10s"
      {% endif %}
      {% if master == 'true' %}
      - name: "{{ cluster_name }}-kube-master"
        checks:
          - args: ["/usr/local/bin/kubernetes-check-node"]
            interval: "30s"
      - name: "{{ cluster_name }}-kube-apiserver"
        check:
          - tcp: localhost:6443
            interval: 10s
            timeout: 3s
      - name: "{{ cluster_name }}-kube-scheduler"
        check:
          - tcp: localhost:10251
            interval: 10s
            timeout: 3s
      - name: "{{ cluster_name }}-kube-controller-manager"
        check:
          - tcp: localhost:10252
            interval: 10s
            timeout: 3s
      {% endif %}
      {% if node == 'true' or master == 'true' %}
      - name: "{{ cluster_name }}-kubelet"
        check:
          - tcp: localhost:10250
            interval: 10s
            timeout: 1s
      - name: "{{ cluster_name }}-kubelet-health"
        check:
          - http: http://localhost:10248/healthz
            method: "GET"
            interval: 30s
            timeout: 5s
      - name: "{{ cluster_name }}-kube-proxy"
        check:
          - tcp: localhost:10256
            interval: 10s
            timeout: 1s
      {% endif %}
      {% if node == 'true' %}
      - name: "{{ cluster_name }}-kube-node"
      {% endif %}

- name: consul | render consul watches
  set_fact:
    consul_watches_render: |
      - type: "keyprefix"
        prefix: "{{ cluster_name }}/cleanup"
        args:
      {% if etcd == 'true' or master == 'true' %}
          - "/usr/local/bin/kubernetes-scripts/cleanup"
      {% else %}
          - "/bin/true"
      {% endif %}

- name: consul | convert from string to service object
  set_fact:
    consul_service: "{{ consul_service_render | from_yaml }}"

- name: consul | convert from string to watches object
  set_fact:
    consul_watches: "{{ consul_watches_render | from_yaml }}"

- name: consul | create requirement file for consul ansible role
  copy:
    content: |
      - src: entercloudsuite.consul
        version: 1.0.2
    dest: /usr/src/cloud/consul_requirement.yml
  tags: image

- name: consul | ensure roles directory exist
  file: path=/usr/src/cloud/roles state=directory
  tags: image

- name: consul | install role entercloudsuite.consul
  shell: |
    source venv/bin/activate
    ansible-galaxy install -r consul_requirement.yml -p roles
  args:
    chdir: /usr/src/cloud
    executable: /bin/bash
    creates: roles/entercloudsuite.consul
  tags: image

- name: consul | start consul role
  include_role:
    name: entercloudsuite.consul
    public: yes
  vars:
    consul_version: 1.4.4
    consul_config_validate: "{{ consul_user_home }}/bin/consul validate -config-format=json %s"
    consul_configs:
      main:
        bind_addr: "{{ ansible_default_ipv4['address'] }}"
        client_addr: 0.0.0.0
        node_name: "{{ ansible_hostname }}"
        data_dir: "{{ consul_data_dir }}"
        encrypt: "{{ consul_encrypt }}"
        datacenter: "{{ consul_datacenter }}"
        enable_syslog: true
        server: false
        ui: true
        enable_script_checks: true
        services: "{{ consul_service }}"
        watches: "{{ consul_watches }}"
        rejoin_after_leave: true
        node_meta:
          image: "{{ image }}"
          flavor: "{{ flavor }}"
        retry_join:
          - "{{ consul }}"

- name: consul | restart consul service
  service: name=consul state=restarted
